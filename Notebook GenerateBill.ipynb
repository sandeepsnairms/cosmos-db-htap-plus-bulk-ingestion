{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read Cosmos OLAP Store\n",
        "## Prerequiste\n",
        "\n",
        "1. Go to your Synapse Analytics workspace.\n",
        "    1. Create a Linked Data connection for your SQL API account.\n",
        "    2. Under the Data blade, select the **+ (plus)** sign.\n",
        "    3. Select the **Connect to external data** option.\n",
        "    4. Now select the **Azure Cosmos DB (SQLAPI)** option.\n",
        "    5. Enter all the information regarding your specific Azure Cosmos DB account either by using the dropdowns or by entering the connection string.\n",
        "    6. Take note of the name you assigned to your Linked Data connection.\n",
        "    1. Use the Linked Data connection name to replace {Linked Synapse DB Name} placeholders in Step #1 and Step #7 below.\n",
        "  \n",
        "2. Test the connection by looking for your database accounts in the Data blade, and under the Linked tab.\n",
        "    - There should be a list that contains all accounts and collections.\n",
        "    - Collections that have an Analytical Store enabled will have a distinctive icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 1: Load Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Read from Cosmos DB analytical store into a Spark DataFrame and display 5 rows from the DataFrame\n",
        "# To select a preferred list of regions in a multi-region Cosmos DB account, add .option(\"spark.cosmos.preferredRegions\", \"<Region1>,<Region2>\")\n",
        "\n",
        "df = spark.read\\\n",
        "    .format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"{Linked Synapse DB Name}\")\\\n",
        "    .option(\"spark.cosmos.container\", \"CallRecords\")\\\n",
        "    .load()\\\n",
        "   \n",
        "    \n",
        "display (df.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 2: View Schema Representation\n",
        "\n",
        "\n",
        "The default option for Azure Cosmos DB CORE (SQL) API, is **Well defined Schema**, but we have used **Full Fidelity Schema** . For more information about schemas representation, click [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#schema-representation) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 3: Unpack the Dataframe\n",
        "For aggregation, a syntax that doesn't explicity mention the datatypes executes without an error. But it is **not recommended!**\n",
        "\n",
        "It runs because Spark automatically flattens the structure into an Array, where it takes each distinct value in the struct dict and applies the aggregation function.\n",
        "\n",
        "However, when we have more than one datatype for the same struct of a property, the implicit conversion by Spark does can cause wrong results.\n",
        "\n",
        "The below code we will:\n",
        "1. Select just the pertinent columns.\n",
        "2. Unpack the struct columns with df.attrname.datatype syntax.\n",
        "3. Rename the columns of the unpacked dataframe\n",
        "4. Cast the DateTime fields from string to timestamp (ISO 8601 and other formats will be read by Synapse as a string datatype. Hence you need to use regular functions to cast it to datetime).\n",
        "5. Filter the dataframe to limit data to JUN2022 BillCycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Select just the pertinent columns and unpack with df.attrname.datatype syntax\n",
        "\n",
        "df_unpacked = df.select(\n",
        "    df.StartDateTime.string,\n",
        "    df.EndDateTime.string,\n",
        "    df.DurationSec.num,\n",
        "    df.CallFrom.string,\n",
        "    df.CallTo.string, \n",
        "    df.CallType.string,\n",
        "    df.CallLocationId.num,\n",
        "    df.BaseLocationId.num,\n",
        "    df.IsRoaming.bool,\n",
        "    df.IsIncoming.bool,\n",
        "    df.SubscriberId.string,\n",
        "    df.BillCycle.string,\n",
        "    df.pk.string\n",
        "    )\n",
        "\n",
        "display (df_unpacked.limit(5))\n",
        "\n",
        "# Rename the columns of the unpacked DataFrame\n",
        "new_column_names = [\n",
        "    'StartDateTime', 'EndDateTime', 'DurationSec', 'CallFrom', 'CallTo','CallType','CallLocationId','BaseLocationId','IsRoaming','IsIncoming','SubscriberId','BillCycle','PK']\n",
        "df_renamed= df_unpacked.toDF(*new_column_names)\n",
        "\n",
        "# cast string to timestamp\n",
        "from pyspark.sql.types import TimestampType\n",
        "df_cast=df_renamed.withColumn(\"StartDateTime\",df_renamed[\"StartDateTime\"].cast(TimestampType()))\n",
        "df_cast=df_cast.withColumn(\"EndDateTime\",df_cast[\"EndDateTime\"].cast(TimestampType()))\n",
        "\n",
        "displayHTML(\"After Renaming Columns and Cast\")\n",
        "display (df_cast.limit(5))\n",
        "\n",
        "# Filter the dataframe to limit data to current Billcycle\n",
        "df_flat= df_cast.filter(col('BillCycle')==\"JUN2022\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 4: Converting Flat Dataframe to Nested Dataframe\n",
        "Copy the call details into a new column called \"Log\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_nested= df_flat.withColumn(\n",
        "  \"Log\",\n",
        "  F.struct(\n",
        "    F.col(\"StartDateTime\").alias(\"StartDateTime\"),\n",
        "    F.col(\"EndDateTime\").alias(\"EndDateTime\"),\n",
        "    F.col(\"DurationSec\").alias(\"DurationSec\"),\n",
        "    F.col(\"CallFrom\").alias(\"CallFrom\"),\n",
        "    F.col(\"CallTo\").alias(\"CallTo\"),\n",
        "    F.col(\"CallType\").alias(\"CallType\")\n",
        "  )\n",
        ")\n",
        "\n",
        "display (df_nested.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 5: Calculate Cost\n",
        "\n",
        "The code below emulates the bill generation using a hyper simplistic algorithm\n",
        "1. Conditonally calculate cost based on 'IsIncoming' column value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Conditional cost calculation based on 'IsIncoming' column value \n",
        "df_calculate=df_nested.withColumn('Cost',\n",
        "              F.when(F.col('IsIncoming') == 0, F.col('DurationSec') * 0.005).otherwise(\n",
        "                F.when(F.col('IsIncoming') == 1, F.col('DurationSec') * 0.001)))\n",
        "\n",
        "\n",
        "display (df_calculate.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 6: Aggregate Data for Bill Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# group by SubscriberId and PK and  then total the Cost\n",
        "df_agg=df_calculate.groupby(\"SubscriberId\",\"PK\").agg(F.collect_list(\"Log\"), F.sum(\"Cost\"))\n",
        "\n",
        "# rename the columns\n",
        "new_column_names = [\n",
        "    'SubscriberId','id', 'Log', 'Cost']\n",
        "df_write= df_agg.toDF(*new_column_names)\n",
        "\n",
        "#reorder the columns\n",
        "df_write=df_write.select( 'id','SubscriberId', 'Cost','Log')\n",
        "\n",
        "display(df_write.limit(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Step 7: Insert data into Cosmos OLTP Store\n",
        "\n",
        "Use dataframe .save to insert the data frame into the Cosmos DB OLTP store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_write.write.format(\"cosmos.oltp\")\\\n",
        "    .option(\"spark.synapse.linkedService\", \"{Linked Synapse DB Name}\")\\\n",
        "    .option(\"spark.cosmos.container\", \"Bills\")\\\n",
        "    .mode('append')\\\n",
        "    .save()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "ad85ed4c-791b-4366-8184-04fd26752763": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "sum",
                "categoryFieldKeys": [],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": []
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [],
              "schema": [],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
