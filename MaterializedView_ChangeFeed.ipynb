{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "cosmosEndpoint = \"https://{Cosmos Account Name}.documents.azure.com:443/\"\n",
        "cosmosMasterKey = \"{Cosmos Primary Key}\"\n",
        "cosmosDatabaseName = \"ContosoMobile\"\n",
        "cosmosContainerName =  \"CallRecords\"\n",
        "cosmosDestContainerName = \"UnBilledCallRecords\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "checkpointLocation = \"/tmp/unbilled_checkpoint\"\n",
        "startOffsetLocation = checkpointLocation + \"/startOffset/0\"\n",
        "startOffsetBackupLocation = checkpointLocation + \"/startOffset/0.bak\"\n",
        "latestOffsetLocation = checkpointLocation + \"/latestOffset/0\"\n",
        "\n",
        "changeFeedCfg = {\n",
        "  \"spark.cosmos.accountEndpoint\" : cosmosEndpoint,\n",
        "  \"spark.cosmos.accountKey\" : cosmosMasterKey,\n",
        "  \"spark.cosmos.database\" : cosmosDatabaseName,\n",
        "  \"spark.cosmos.container\" : cosmosContainerName,\n",
        "  \"spark.cosmos.read.inferSchema.enabled\" : \"true\",   \n",
        "  \"spark.cosmos.changeFeed.startFrom\" : \"Now\",\n",
        "  \"spark.cosmos.changeFeed.mode\" : \"Incremental\",\n",
        "  \"spark.cosmos.read.partitioning.strategy\": \"Restrictive\",\n",
        "  \"spark.cosmos.partitioning.targetedCount\": \"1\",\n",
        "  \"spark.cosmos.changeFeed.itemCountPerTriggerHint\" : \"100000\",\n",
        "  \"spark.cosmos.changeFeed.batchCheckpointLocation\" : checkpointLocation\n",
        "  # optional configuration for throughput control\n",
        "  # \"spark.cosmos.throughputControl.enabled\" : \"true\",\n",
        "  # \"spark.cosmos.throughputControl.name\" : \"SourceContainerThroughputControl\",\n",
        "  # \"spark.cosmos.throughputControl.targetThroughputThreshold\" : \"0.30\", \n",
        "  # \"spark.cosmos.throughputControl.globalControl.database\" : \"database-v4\", \n",
        "  # \"spark.cosmos.throughputControl.globalControl.container\" : \"ThroughputControl\"\n",
        "}\n",
        "\n",
        "# patch increment operation\n",
        "cfgIncrement_Incoming = {\"spark.cosmos.accountEndpoint\": cosmosEndpoint,\n",
        "            \"spark.cosmos.accountKey\": cosmosMasterKey,\n",
        "            \"spark.cosmos.database\": cosmosDatabaseName,\n",
        "            \"spark.cosmos.container\": cosmosDestContainerName,\n",
        "            \"spark.cosmos.write.strategy\": \"ItemPatch\",\n",
        "            \"spark.cosmos.write.bulk.enabled\": \"true\",\n",
        "            \"spark.cosmos.write.patch.columnConfigs\": \"[col(incoming).op(increment),col(outgoing).op(increment),col(cnt).op(increment)]\",\n",
        "            \n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "\n",
        "try:\n",
        "  mssparkutils.fs.ls(checkpointLocation)\n",
        "except Exception:\n",
        "  print(\"Checkpoint location doesn't exist yet.\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "from notebookutils import mssparkutils\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "sourceRecordCount = 0\n",
        "targetRecordCount = 0\n",
        "emptyCount = 0\n",
        "maxEmptyBeforeStopListening = 3\n",
        "while (emptyCount < maxEmptyBeforeStopListening):\n",
        "  changeFeed_DF = spark.read.format(\"cosmos.oltp.changefeed\") \\\n",
        "      .options(**changeFeedCfg) \\\n",
        "      .load() \n",
        "  changeFeed_DF.persist()\n",
        "  sourceRecordCount = changeFeed_DF.count() \n",
        "  print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \": Retrieved \" + str(sourceRecordCount) + \" records from change feed.\")\n",
        "  if (sourceRecordCount == 0):\n",
        "    emptyCount += 1\n",
        "    sleepTime = emptyCount * 5\n",
        "    if (emptyCount < maxEmptyBeforeStopListening):\n",
        "      print(\"Sleeping \" + str(sleepTime) + \" seconds...\")\n",
        "      time.sleep(sleepTime)\n",
        "  else:\n",
        "    emptyCount = 0    \n",
        "\n",
        "    changeFeed_transformed_DF = changeFeed_DF \\\n",
        "      .withColumn('outgoingTemp',\n",
        "              F.when(F.col('IsIncoming') == 0, F.col('DurationSec') *1).otherwise(\n",
        "                F.when(F.col('IsIncoming') == 1, F.col('DurationSec') * 0))) \\\n",
        "      .withColumn('incomingTemp',\n",
        "              F.when(F.col('IsIncoming') == 1, F.col('DurationSec') *1).otherwise(\n",
        "                F.when(F.col('IsIncoming') == 0, F.col('DurationSec') * 0))) \\\n",
        "      .drop(\"id\") \\\n",
        "      .withColumnRenamed('SubscriberId', 'id') \\\n",
        "      .groupBy(\"id\") \\\n",
        "      .agg(F.sum(\"incomingTemp\").alias(\"incoming\"),F.sum(\"outgoingTemp\").alias(\"outgoing\"),F.count(\"*\").alias(\"cnt\"))\n",
        "\n",
        "    targetRecordCount=changeFeed_transformed_DF.count()\n",
        "    changeFeed_transformed_DF \\\n",
        "        .write \\\n",
        "        .format(\"cosmos.oltp\") \\\n",
        "        .mode(\"Append\") \\\n",
        "        .options(**cfgIncrement_Incoming) \\\n",
        "        .save()\n",
        "\n",
        "    print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \": Processed \" + str(targetRecordCount) + \" subscriber records...\")    \n",
        "\n",
        "  print(\"Updating checkpoints...\")\n",
        "  # Start offset (and backup) might not exist yet\n",
        "  try:\n",
        "    mssparkutils.fs.rm(startOffsetBackupLocation, False)\n",
        "  except Exception:\n",
        "    print(\"StartOffset backup file doesn't exist yet.\")  \n",
        "\n",
        "  try:    \n",
        "    mssparkutils.fs.cp(startOffsetLocation, startOffsetBackupLocation, False)\n",
        "  except Exception:\n",
        "    print(\"StartOffset doesn't exist.\")    \n",
        "\n",
        "  mssparkutils.fs.mv(latestOffsetLocation, startOffsetLocation, True, True)\n",
        "  \n",
        "  print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \": Processed records and updated checkpoint.\")\n",
        "\n",
        "  changeFeed_DF.unpersist()  \n",
        "\n",
        "print(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \": STOPPED LISTENING FOR MORE CHANGES!\")  "
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
